{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yav3-EX2beW",
        "outputId": "35c8b8d9-217c-4729-cdce-1e5882fb04fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=edcac57718d5d8f4ea4440f57438f51cfb479d1fb7a953f0af23ca05807de2d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# prompt: I need to import pyspark library and load sample file\n",
        "\n",
        "!pip install pyspark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Configure Spark properties\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Segwise\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.cores\", \"2\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "pU-ShP2s2ijG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('/content/drive/MyDrive/google-play-dataset-by-tapivedotcom.parquet')\n",
        "\n",
        "# read your csv I have converted to parquet and reading it\n",
        "#playstore.csv\n"
      ],
      "metadata": {
        "id": "WQPpFvJR2lHg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define numerical fields for binning\n",
        "from pyspark.sql.functions import col, when, expr\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "from pyspark.sql.functions import udf, broadcast, col, count\n",
        "\n",
        "import math\n",
        "\n",
        "numerical_fields = [\n",
        "    'minInstalls',\n",
        "    'price',\n",
        "    'offersIAP',\n",
        "    'ratings',\n",
        "    'adSupported',\n",
        "    'containsAds',\n",
        "    'reviews',\n",
        "    'free',\n",
        "    'score',\n",
        "    'releasedDay',\n",
        "    'releasedYear',\n",
        "    'maxprice',\n",
        "    'histogram1',\n",
        " 'histogram2',\n",
        " 'histogram3',\n",
        " 'histogram4',\n",
        " 'histogram5',\n",
        "]\n",
        "df = df.na.fill(0, subset=numerical_fields)\n",
        "\n",
        "for field in numerical_fields:\n",
        "    df = df.withColumn(field, col(field).cast('double'))\n",
        "# Function to generate bin size for a column\n",
        "def gen_bin_size(df, column, num_bins=5):\n",
        "    min_val, max_val = df.selectExpr(f\"min({column})\", f\"max({column})\").collect()[0]\n",
        "    bin_size = max(math.ceil((max_val - min_val) / num_bins), 1)\n",
        "    return bin_size\n",
        "\n",
        "def gen_bin_size_dynamic(df, column, bin_percentage=5):\n",
        "    min_val, max_val = df.selectExpr(f\"min({column})\", f\"max({column})\").collect()[0]\n",
        "    bin_size = max(math.ceil((max_val - min_val) * bin_percentage / 100), 1)\n",
        "    return bin_size\n",
        "\n",
        "\n",
        "# Define a function for binning numerical fields\n",
        "def bin_numerical_fields(df, fields, num_bins=5):\n",
        "    bin_columns = []\n",
        "    for field in fields:\n",
        "        # Check if the field has at least 20 unique values\n",
        "        unique_values = df.select(field).distinct().count()\n",
        "        if unique_values >= 20:\n",
        "            # Calculate bin size\n",
        "            bin_size = gen_bin_size_dynamic(df, field, num_bins)\n",
        "            print(f\"Field: {field}, Bin Size: {bin_size}\")\n",
        "\n",
        "            # Create Bucketizer to create bins\n",
        "            max_val = int(df.selectExpr(f\"max({field})\").collect()[0][0])\n",
        "            splits = [float(i) for i in range(0, max_val, bin_size)] + [float(max_val + 1)]  # Add one more split\n",
        "\n",
        "            intervals = []\n",
        "            for i in range(0, len(splits)-1):\n",
        "                intervals.append(f\"({splits[i]}, {splits[i+1]}]\")\n",
        "            mapping = spark.sparkContext.broadcast(intervals)\n",
        "\n",
        "            def get_binns(values):\n",
        "                def f(x):\n",
        "                    if x is None:\n",
        "                        return values[int(0)]\n",
        "                    else:\n",
        "                        return values[int(x)]\n",
        "                return udf(f)\n",
        "            bucketizer = Bucketizer(splits=splits, inputCol=field, outputCol=field + '_bin')\n",
        "\n",
        "            # Transform the data and add bin column to the DataFrame\n",
        "            df = bucketizer.transform(df)\n",
        "            df = df.withColumn(field+'_bin', get_binns(mapping.value)(col(field+'_bin')))\n",
        "\n",
        "            bin_columns.append(field + \"_bin\")\n",
        "\n",
        "    return df, bin_columns\n",
        "\n",
        "# Bin numerical fields\n",
        "binned_df, bin_columns = bin_numerical_fields(df, numerical_fields)\n",
        "\n",
        "# Define categorical fields for filtering\n",
        "categorical_fields = ['genre']\n",
        "\n",
        "# Filter out combinations smaller than 2% of total volume\n",
        "filtered_df = binned_df\n",
        "for field in categorical_fields:\n",
        "    field_counts = binned_df.groupBy(field).count()\n",
        "    total_count = binned_df.count()\n",
        "    filtered_df = filtered_df.join(field_counts, field, \"left_outer\").filter(col(\"count\") >= 0.02 * total_count)\n",
        "\n",
        "\n",
        "# Define columns for the final output\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WohOxnff2nNU",
        "outputId": "fad620f5-a94d-4f7c-a992-2afff37a757b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Field: minInstalls, Bin Size: 500000000\n",
            "Field: price, Bin Size: 55\n",
            "Field: ratings, Bin Size: 8320873\n",
            "Field: reviews, Bin Size: 219701\n",
            "Field: score, Bin Size: 1\n",
            "Field: releasedDay, Bin Size: 2\n",
            "Field: maxprice, Bin Size: 52\n",
            "Field: histogram1, Bin Size: 2663685\n",
            "Field: histogram2, Bin Size: 230845\n",
            "Field: histogram3, Bin Size: 402898\n",
            "Field: histogram4, Bin Size: 841874\n",
            "Field: histogram5, Bin Size: 5948483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping unrequired column\n",
        "lio = []\n",
        "for i in filtered_df.columns:\n",
        "  if '_bin' in i or i=='count':\n",
        "    pass\n",
        "  else:\n",
        "    lio.append(i)\n",
        "filtered_df = filtered_df.drop(*lio)"
      ],
      "metadata": {
        "id": "m9cr4vSi222G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "for col in filtered_df.columns[:-1]:  # Exclude the 'finalCount' column\n",
        "    filtered_df = filtered_df.withColumn(col, F.translate(col, '()', '[]'))"
      ],
      "metadata": {
        "id": "qmzFmcXw4QsE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_columns = bin_columns + categorical_fields + [\"count\"]\n"
      ],
      "metadata": {
        "id": "Z9KSN6O83N6f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h5StoVf8ka9",
        "outputId": "c184a22a-b7da-4d05-a93e-de0a1be3ceac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['minInstalls_bin',\n",
              " 'price_bin',\n",
              " 'ratings_bin',\n",
              " 'reviews_bin',\n",
              " 'score_bin',\n",
              " 'releasedDay_bin',\n",
              " 'maxprice_bin',\n",
              " 'histogram1_bin',\n",
              " 'histogram2_bin',\n",
              " 'histogram3_bin',\n",
              " 'histogram4_bin',\n",
              " 'histogram5_bin']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "# due to memory issues limiting the subset\n",
        "subset_limit = 10\n",
        "# If you decrease the subset_limit the subset will be more\n",
        "numerical_subsets = list(chain.from_iterable(combinations(bin_columns, r) for r in range(0,len(bin_columns) + 1,subset_limit)))\n"
      ],
      "metadata": {
        "id": "rHWnOxL23Ug5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create empty df\n",
        "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
        "\n",
        "final_schema = StructType([\n",
        "    StructField(\"minInstalls_bin\", StringType(), True),\n",
        "    StructField(\"price_bin\", StringType(), True),\n",
        "    StructField(\"ratings_bin\", StringType(), True),\n",
        "    StructField(\"reviews_bin\", StringType(), True),\n",
        "    StructField(\"score_bin\", StringType(), True),\n",
        "    StructField(\"releasedDay_bin\", StringType(), True),\n",
        "    StructField(\"maxprice_bin\", StringType(), True),\n",
        "    StructField(\"histogram1_bin\", StringType(), True),\n",
        "    StructField(\"histogram2_bin\", StringType(), True),\n",
        "    StructField(\"histogram3_bin\", StringType(), True),\n",
        "    StructField(\"histogram4_bin\", StringType(), True),\n",
        "    StructField(\"histogram5_bin\", StringType(), True),\n",
        "    StructField(\"finalCount\", IntegerType(), True)\n",
        "])\n",
        "final_df = spark.createDataFrame([], schema=final_schema)"
      ],
      "metadata": {
        "id": "L_wGjP4l6RKy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_rows = []\n",
        "c =0\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "\n",
        "for fil in numerical_subsets[::-1]:\n",
        "  subset_df = filtered_df.groupBy(*fil).agg(F.sum(\"count\").alias(\"finalCount\"))\n",
        "  # print(subset_df.columns)\n",
        "  for i in bin_columns:\n",
        "    if i not in subset_df.columns:\n",
        "      subset_df = subset_df.withColumn(i ,lit(None))\n",
        "\n",
        "\n",
        "  final_df = final_df.union(subset_df)\n",
        "  # final_df = final_df.repartition(100)\n",
        "  c+=1\n"
      ],
      "metadata": {
        "id": "YvJ5kCqu3nq6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "Gmr7cWkD97Fu",
        "outputId": "67fcb8db-b9a9-4c09-aa43-42b7d31460ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bcee4b6558b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.selectExpr(\n",
        "    \"CASE WHEN minInstalls_bin IS NOT NULL THEN concat('minInstalls=', cast(minInstalls_bin as string)) END as a1\",\n",
        "    \"CASE WHEN price_bin IS NOT NULL THEN concat('price=', cast(price_bin as string)) END as a2\",\n",
        "    \"CASE WHEN ratings_bin IS NOT NULL THEN concat('ratings=', cast(ratings_bin as string)) END as a3\",\n",
        "    \"CASE WHEN reviews_bin IS NOT NULL THEN concat('reviews=', cast(reviews_bin as string)) END as a4\",\n",
        "    \"CASE WHEN score_bin IS NOT NULL THEN concat('score=', cast(score_bin as string)) END as a5\",\n",
        "    \"CASE WHEN releasedDay_bin IS NOT NULL THEN concat('releasedDay=', cast(releasedDay_bin as string)) END as a6\",\n",
        "    \"CASE WHEN maxprice_bin IS NOT NULL THEN concat('maxprice=', cast(maxprice_bin as string)) END as a7\",\n",
        "    \"CASE WHEN histogram1_bin IS NOT NULL THEN concat('histogram1=', cast(histogram1_bin as string)) END as a8\",\n",
        "    \"CASE WHEN histogram2_bin IS NOT NULL THEN concat('histogram2=', cast(histogram1_bin as string)) END as a9\",\n",
        "    \"CASE WHEN histogram3_bin IS NOT NULL THEN concat('histogram3=', cast(histogram1_bin as string)) END as a10\",\n",
        "    \"CASE WHEN histogram4_bin IS NOT NULL THEN concat('histogram4=', cast(histogram4_bin as string)) END as a11\",\n",
        "    \"CASE WHEN histogram5_bin IS NOT NULL THEN concat('histogram5=', cast(histogram5_bin as string)) END as a12\",\n",
        "    \"cast(finalCount as string) as finalCount\"\n",
        ").filter(\"a1 IS NOT NULL or a2 IS NOT NULL or a3 IS NOT NULL or a4 IS NOT NULL or a5 IS NOT NULL \" +\n",
        "         \"or a6 IS NOT NULL or a7 IS NOT NULL or a8 IS NOT NULL or a9 IS NOT NULL or a10 IS NOT NULL or a11 IS NOT NULL or a12 IS NOT NULL\")\n"
      ],
      "metadata": {
        "id": "qWYeaH3b_M7D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7Wtut_b_ie6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ll = []\n",
        "# for i in range(c):\n",
        "#   s = f'/content/spark_outputt/{i}/'\n",
        "#   ll.append(s)\n",
        "# spa = spark.read.csv(ll,header=True)\n",
        "\n",
        "# output_df1 = spa.selectExpr(\n",
        "#     \"concat('minInstalls=', cast(minInstalls_bin as string)) as a1\",\n",
        "#     \"concat('price=', cast(price_bin as string)) as a2\",\n",
        "#     \"concat('ratings=', cast(ratings_bin as string)) as a3\",\n",
        "#     \"concat('reviews=', cast(reviews_bin as string)) as a4\",\n",
        "#     \"concat('score=', cast(score_bin as string)) as a5\",\n",
        "#     \"concat('releasedDay=', cast(releasedDay_bin as string)) as a6\",\n",
        "#     \"concat('maxprice=', cast(maxprice_bin as string)) as a7\",\n",
        "#     \"concat('histogram1=', cast(histogram1_bin as string)) as a8\",\n",
        "#     \"concat('histogram4=', cast(histogram4_bin as string)) as a9\",\n",
        "#     \"concat('histogram5=', cast(histogram5_bin as string)) as a10\",\n",
        "#     \"cast(finalCount as string) as finalCount\"\n",
        "#   )\n",
        "final_df = final_df.selectExpr(\n",
        "    \"concat_ws(';', a1, a2, a3, a4, a5, a6, \" +\n",
        "    \"a7, a8, a9, a10,a11,a12) as properties\",\n",
        "    \"cast(finalCount as string) as value\"\n",
        "  )"
      ],
      "metadata": {
        "id": "DQeGVdcI8Sgn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.write.options(header='True', delimiter=',').csv(f\"spark_out/\")"
      ],
      "metadata": {
        "id": "jrbBTRNj4NIe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}